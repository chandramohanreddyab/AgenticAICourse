{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cca932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import langchain, langchain_experimental,langchain_core,langchain_community, langchain_experimental\n",
    "#loading the environment variables for API keys and also setting other env variable for longchain tracing\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"AgenticAI\"\n",
    "os.environ[\"LANGSMITH_TRACING\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef8e591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets send q query to ChatGPT model\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model='gpt-4o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a01cfa",
   "metadata": {},
   "source": [
    "## Simple query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16647a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agentic AI refers to artificial intelligence systems that are designed to operate with a degree of autonomy, making decisions and acting on behalf of their users or creators. These systems are typically capable of perceiving their environment, processing information, and performing actions to achieve specific goals or tasks. Agentic AI often involves the integration of machine learning, natural language processing, and other AI technologies to enable the system to perform complex tasks with minimal human intervention.\\n\\nThe concept of agentic AI contrasts with other forms of AI that are more passive or merely provide information without taking autonomous actions. Agentic AI systems might include robotic systems, autonomous vehicles, or virtual assistant software that can perform tasks ranging from scheduling meetings to performing complex logistics operations. The development and deployment of agentic AI involve careful consideration of ethical, security, and operational issues, as these systems have the potential to significantly impact their environments and the people within them.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=llm.invoke(\"What is agentic AI\")\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcd4c89",
   "metadata": {},
   "source": [
    "## Prompt Engineering using the LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b9d2e5",
   "metadata": {},
   "source": [
    "Prompt templates help to translate user input and parameters into instructions for a language model. This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.\n",
    "\n",
    "Prompt Templates take as input a dictionary, where each key represents a variable in the prompt template to fill in.\n",
    "\n",
    "Prompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or a list of messages. The reason this PromptValue exists is to make it easy to switch between strings and messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf02b88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me a joke about cats')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#These prompt templates are used to format a single string, and generally are used for simpler inputs. For example, a common way to construct and use a PromptTemplate is as follows:\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "prompt_template.invoke({\"topic\": \"cats\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4263f848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#These prompt templates are used to format a list of messages. These \"templates\" consist of a list of templates themselves. For example, a common way to construct and use a ChatPromptTemplate is as follows:\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "prompt_template.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59bcfb7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This prompt template is responsible for adding a list of messages in a particular place. In the above ChatPromptTemplate, we saw how we could format two messages, each one a string. \n",
    "# But what if we wanted the user to pass in a list of messages that we would slot into a particular spot? This is how you use MessagesPlaceholder.\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "\n",
    "\n",
    "prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc5ce1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This will produce a list of two messages, the first one being a system message, and the second one being the HumanMessage we passed in. \n",
    "# If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in). \n",
    "# This is useful for letting a list of messages be slotted into a particular spot.\n",
    "\n",
    "\n",
    "#An alternative way to accomplish the same thing without using the MessagesPlaceholder class explicitly is:\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"placeholder\", \"{msgs}\") # <-- This is the changed part\n",
    "])\n",
    "prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0375a8e2",
   "metadata": {},
   "source": [
    "## How use Prompts in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdc4af53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 31, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-BdEZ1CfobyxBtYl3uyDzEzx0QwXc2', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--e6a21d39-13fb-4298-8f79-8f0888ed09d9-0', usage_metadata={'input_tokens': 31, 'output_tokens': 5, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_result=llm.invoke(messages)\n",
    "ai_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3035607a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ich liebe das Programmieren.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chaining the models using prompts\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "response=chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"German\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c9a2e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the bear get kicked out of the campground?\\n\\nBecause he was always \"paw-liticking\"!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e5abcb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': 'Why was the cat sitting on the computer? Because it wanted to keep an eye on the mouse!'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Json output formattor instead of the String?\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "promt_json= PromptTemplate(template=\"Answer the user queries. \\n Tell me a joke about {topic} \\n give ther response in {format}\",\n",
    "                                    input_variables=['topic'],\n",
    "                                    partial_variables={'format': JsonOutputParser().get_format_instructions()}\n",
    "                                    )\n",
    "\n",
    "chain_json = promt_json | llm | JsonOutputParser()\n",
    "chain_json.invoke({\"topic\": \"cat\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1775931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': \"Why did the dog sit in the shade? Because he didn't want to be a hot dog!\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatpromt_json= ChatPromptTemplate([(\"system\",\"You are a stand up camedian /n and give response in {output_format}\"),\n",
    "                                    ('user',\"tell me joke about {topic}\")], \n",
    "                                    input_variables=['topic'],\n",
    "                                    partial_variables={'output_format': JsonOutputParser().get_format_instructions()}\n",
    "                                    )\n",
    "chain_json = chatpromt_json | llm | JsonOutputParser()\n",
    "chain_json.invoke({\"topic\": \"dog\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "094d8b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'product_id': 'ELEC001',\n",
       "  'product_name': 'Smartphone ProMax 12',\n",
       "  'product_price': 999.99},\n",
       " {'product_id': 'ELEC002',\n",
       "  'product_name': 'Ultra-HD 4K LED TV 55\"',\n",
       "  'product_price': 1499.99},\n",
       " {'product_id': 'ELEC003',\n",
       "  'product_name': 'Noise-Canceling Headphones',\n",
       "  'product_price': 349.95},\n",
       " {'product_id': 'ELEC004',\n",
       "  'product_name': 'Laptop Core i7 16GB',\n",
       "  'product_price': 1199.89},\n",
       " {'product_id': 'ELEC005',\n",
       "  'product_name': 'Wireless Bluetooth Speaker',\n",
       "  'product_price': 129.99},\n",
       " {'product_id': 'ELEC006',\n",
       "  'product_name': 'Smartwatch Series 6',\n",
       "  'product_price': 399.99},\n",
       " {'product_id': 'ELEC007',\n",
       "  'product_name': 'Gaming Console Pro',\n",
       "  'product_price': 499.99},\n",
       " {'product_id': 'ELEC008',\n",
       "  'product_name': 'Tablet 10.5\" 128GB',\n",
       "  'product_price': 599.99},\n",
       " {'product_id': 'ELEC009',\n",
       "  'product_name': 'Digital Camera 24MP',\n",
       "  'product_price': 799.95},\n",
       " {'product_id': 'ELEC010',\n",
       "  'product_name': 'Home Assistant Smart Hub',\n",
       "  'product_price': 199.99}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using pydantic for the data type validation\n",
    "product_promt=ChatPromptTemplate([('system','you are a product DB and returns top {num_products}, product ID, product name and Product price, returns in {format}'),\n",
    "                                                (\"user\",'give 10 product list from product category type {product_category}')]\n",
    "                                                )\n",
    "product_chain= product_promt | llm | JsonOutputParser()\n",
    "product_chain.invoke({\"product_category\":'Electornings',\n",
    "                      \"num_products\":10,\n",
    "                      \"format\":JsonOutputParser().get_format_instructions()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86610086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'product_id': 'E1001',\n",
       "  'product_description': 'Smartphone XYZ Pro',\n",
       "  'product_price': 999.99},\n",
       " {'product_id': 'E1002',\n",
       "  'product_description': '4K Ultra HD Smart TV 55in',\n",
       "  'product_price': 549.99},\n",
       " {'product_id': 'E1003',\n",
       "  'product_description': 'Bluetooth Noise Cancelling Headphones',\n",
       "  'product_price': 199.99},\n",
       " {'product_id': 'E1004',\n",
       "  'product_description': 'Laptop Pro 16GB RAM, 512GB SSD',\n",
       "  'product_price': 1299.99},\n",
       " {'product_id': 'E1005',\n",
       "  'product_description': 'Smartwatch Series 6',\n",
       "  'product_price': 399.99},\n",
       " {'product_id': 'E1006',\n",
       "  'product_description': 'Wireless Home Security Camera',\n",
       "  'product_price': 89.99},\n",
       " {'product_id': 'E1007',\n",
       "  'product_description': 'Portable Bluetooth Speaker',\n",
       "  'product_price': 49.99},\n",
       " {'product_id': 'E1008',\n",
       "  'product_description': 'Gaming Console X',\n",
       "  'product_price': 499.99},\n",
       " {'product_id': 'E1009',\n",
       "  'product_description': 'Digital SLR Camera 24MP',\n",
       "  'product_price': 749.99},\n",
       " {'product_id': 'E1010',\n",
       "  'product_description': 'Smart Home Thermostat',\n",
       "  'product_price': 249.99}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Validating ouput with Pydantic\n",
    "\n",
    "from pydantic import BaseModel\n",
    "class Product(BaseModel):\n",
    "    product_id: str\n",
    "    product_description: str\n",
    "    product_price: float\n",
    "\n",
    "json_parser= JsonOutputParser(pydantic_object=Product)\n",
    "\n",
    "product_chain= product_promt | llm | json_parser\n",
    "product_chain.invoke({\"product_category\":'Electronincs',\n",
    "                      \"num_products\":10,\n",
    "                      \"format\":json_parser.get_format_instructions()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f303ea05",
   "metadata": {},
   "source": [
    "### Coersion\n",
    "We can even combine this chain with more runnables to create another chain. This may involve some input/output formatting using other types of runnables, depending on the required inputs and outputs of the chain components..-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198800a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Humor is subjective, so what one person finds funny, another might not. The joke you provided is a play on words, using \"bear feet\" to humorously sound like \"bare feet,\" which is a common trait of bears. People who enjoy puns might find this joke amusing due to the clever wordplay.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For example, let's say we wanted to compose the joke generating chain with another chain that evaluates whether or not the generated joke was funny.\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\"is this a funny joke? {joke}\")\n",
    "composed_chain = {\"joke\": chain} | analysis_prompt | llm | StrOutputParser()\n",
    "composed_chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbc9ebd",
   "metadata": {},
   "source": [
    "Note: The information here refers to parsers that take a text output from a model try to parse it into a more structured representation. More and more models are supporting function (or tool) calling, which handles this automatically. It is recommended to use function/tool calling rather than output parsing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971cc047",
   "metadata": {},
   "source": [
    "## Tool Calling\n",
    "#not required in this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d10f7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_yCYluuHss61Qk1XRiabIfwxS', 'function': {'arguments': '{\"location\":\"San Francisco, CA\"}', 'name': 'GetWeather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 68, 'total_tokens': 85, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-BdDdVTHHuwiHtKbCzdJ2BpumqivUq', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--cbd462d9-da97-4070-a84a-6c6d782a298a-0', tool_calls=[{'name': 'GetWeather', 'args': {'location': 'San Francisco, CA'}, 'id': 'call_yCYluuHss61Qk1XRiabIfwxS', 'type': 'tool_call'}], usage_metadata={'input_tokens': 68, 'output_tokens': 17, 'total_tokens': 85, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "#String Validation\n",
    "class GetWeather(BaseModel):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
    "\n",
    "\n",
    "llm_with_tools = llm.bind_tools([GetWeather])\n",
    "weather = llm_with_tools.invoke(\"what is the weather like in San Francisco\")\n",
    "weather.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5161d9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvagentic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

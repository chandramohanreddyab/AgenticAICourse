{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6acde3a2",
   "metadata": {},
   "source": [
    "# Gettiing Start LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5aa7d4",
   "metadata": {},
   "source": [
    "LangChain is an open-source framework that helps developers build applications powered by large language models (LLMs). It provides necessary tools and components to connect LLMs to external data sources, memory, and other systems, making it easier to create complex applications such as chatbots, virtual assistants, and document analysis tools. As the name itself suggest Lang--> Language Models are chained with other components such as prompts, and output processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27286c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # you can pass the absolute path to .env file\n",
    "\n",
    "#store keys in the OS env\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "## Langsmith Tracking And Tracing\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"AgentiAICourse_01\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac4c523",
   "metadata": {},
   "source": [
    "## Simple Query To LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef8e591d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x10b5778c0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x10be00440> root_client=<openai.OpenAI object at 0x10b426ba0> root_async_client=<openai.AsyncOpenAI object at 0x10b577a10> model_name='o3-mini' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "#Lets send query to ChatGPT model with simple query\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"o3-mini\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4035d2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Agentic AI\" generally refers to artificial intelligence systems that are designed to operate with a degree of autonomy, behaving like agents—entities that can perceive their environment, reason about it, and take actions toward goals. Here are some aspects of the concept:\n",
      "\n",
      "1. Autonomy and Decision-Making: Unlike simple, rule-based systems, an agentic AI is capable of making decisions on its own, often without continuous human oversight. It can set priorities, evaluate options, and act in the world to pursue its objectives.\n",
      "\n",
      "2. Goal-Directed Behavior: Agentic AI systems are typically goal-oriented. They are programmed (or learn) to achieve specific outcomes and will take steps they calculate to be effective in reaching their goals—sometimes even adapting their plans if conditions change.\n",
      "\n",
      "3. Interaction with the Environment: These AI systems are designed to engage with complex environments, perceiving inputs (which could come via sensors, data feeds, or other channels), processing this information, and taking actions that affect the environment. Examples might include autonomous vehicles navigating traffic or robotic assistants performing tasks in dynamic settings.\n",
      "\n",
      "4. Emergent Complexity: In some discussions, the term \"agentic\" is used to describe AI that appears to exhibit behavior more akin to human or animal agency. This suggests that even if the AI isn’t conscious or self-aware, its decision-making process and actions are sufficiently complex and independent that it resembles an \"agent\" acting in the world.\n",
      "\n",
      "5. Discussions Around Safety and Ethics: Because agentic AI acts autonomously, there is significant research and debate concerning its alignment with human values, ethical guidelines, and safety controls. Ensuring that an agentic AI’s goals remain beneficial to humans is a major focus in AI safety research, especially as systems become more advanced.\n",
      "\n",
      "In summary, agentic AI is about building systems that not only process information but also act independently to achieve goals, often in complex, real-world environments. It’s a concept that bridges the gap between traditional software programs and systems that exhibit behaviors one might associate with intelligent agents.\n"
     ]
    }
   ],
   "source": [
    "result=llm.invoke(\"What is agentic AI\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a872b07c",
   "metadata": {},
   "source": [
    "##### What if the input query is formatted, also give context to LLM to behave in certain way ?  like giving output in 3 sentences, or to act like Programming developer ? This can be done using the prompt engineering and LangChain library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcd4c89",
   "metadata": {},
   "source": [
    "## Prompt Engineering using the LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b9d2e5",
   "metadata": {},
   "source": [
    "Prompt templates help to translate user input and parameters into instructions for a language model. This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.\n",
    "\n",
    "Prompt Templates take as input a dictionary, where each key represents a variable in the prompt template to fill in.\n",
    "\n",
    "Prompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or a list of messages. The reason this PromptValue exists is to make it easy to switch between strings and messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf02b88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me a joke about cats')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#These prompt templates are used to format a single string, and generally are used for simpler inputs. For example, a common way to construct and use a PromptTemplate is as follows:\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "#simple single message\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "prompt_template.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4263f848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#These prompt templates are used to format a list of messages as well like a chat using the list of templates. For example, a common way to construct and use a ChatPromptTemplate is as follows:\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "prompt_template.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59bcfb7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This prompt template is responsible for adding a list of messages in a particular place. In the above ChatPromptTemplate, we saw how we could format two messages, each one a string. \n",
    "# But what if we wanted the user to pass in a list of messages that we would slot into a particular spot? This is how you use MessagesPlaceholder.\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "\n",
    "\n",
    "prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc5ce1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This will produce a list of two messages, the first one being a system message, and the second one being the HumanMessage we passed in. \n",
    "# If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in). \n",
    "# This is useful for letting a list of messages be slotted into a particular spot.\n",
    "\n",
    "\n",
    "#An alternative way to accomplish the same thing without using the MessagesPlaceholder class explicitly is:\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"placeholder\", \"{msgs}\") # <-- This is the changed part\n",
    "])\n",
    "prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0375a8e2",
   "metadata": {},
   "source": [
    "## How use Prompts in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdc4af53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'aime la programmation.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 210, 'prompt_tokens': 30, 'total_tokens': 240, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 192, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o3-mini-2025-01-31', 'system_fingerprint': 'fp_d83b50479d', 'id': 'chatcmpl-CkTIVs1Xpt6XDJ5c1XesN8eh4oQUM', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--ed06bd62-e45e-42fa-9c67-6ceceda23f9e-0', usage_metadata={'input_tokens': 30, 'output_tokens': 210, 'total_tokens': 240, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 192}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_result=llm.invoke(messages)\n",
    "ai_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3035607a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ich liebe es, zu programmieren.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chaining the models using prompts\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "response=chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"German\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dea9cb",
   "metadata": {},
   "source": [
    "#### Ok, So far we have seen how can we engineer the input to LLMs, can we engineer the outputs as well ? This can be done using the output parsers and instructing LLM to give output in specific format for better processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c9a2e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure! Here's one:\\n\\nWhat do you call a bear with no teeth?\\nA gummy bear!\\n\\nI hope that gave you a little chuckle!\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e5abcb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': 'Why did the cat sit on the computer? Because it wanted to keep an eye on the mouse!'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Json output formattor instead of the String? instead of using ChatTemplate we can using Prompt template for better data engineering between the langchain input,llm and output objects\n",
    "promt_json= PromptTemplate(template=\"Answer the user queries. \\n Tell me a joke about {topic} \\n give ther response in {format}\",\n",
    "                                    input_variables=['topic'],\n",
    "                                    partial_variables={'format': JsonOutputParser().get_format_instructions()}\n",
    "                                    )\n",
    "\n",
    "chain_json = promt_json | llm | JsonOutputParser()\n",
    "chain_json.invoke({\"topic\": \"cat\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1775931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': 'What do you call a dog that can perform magic tricks? A labracadabrador!'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if it is chattemplate you have to give instructions in list of prompt templates \n",
    "chatpromt_json= ChatPromptTemplate([(\"system\",\"You are a stand up camedian /n and give response in {output_format}\"),\n",
    "                                    ('user',\"tell me joke about {topic}\")], \n",
    "                                    input_variables=['topic'],\n",
    "                                    partial_variables={'output_format': JsonOutputParser().get_format_instructions()}\n",
    "                                    )\n",
    "chain_json = chatpromt_json | llm | JsonOutputParser()\n",
    "chain_json.invoke({\"topic\": \"dog\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5651070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Humor is pretty subjective—what cracks one person up might leave another cold. I think the joke’s charm lies in its play on words. It uses the exercise idea (“jumping to conclusions” and “quick on the mental treadmill”) to create a pun that links physical activity with a mental shortcut. If you’re into puns or appreciate clever wordplay, you might find it really funny. Others might think it’s more of a groaner than a belly laugh. It all depends on your sense of humor!'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For example, let's say we wanted to compose the joke generating chain with another chain that evaluates whether or not the generated joke was funny.\n",
    "analyse_prompt= ChatPromptTemplate.from_template(\" is this a really funny joke ? {joke_input}\")\n",
    "composed_chain= {\"joke_input\":chain} | analyse_prompt| llm | StrOutputParser()\n",
    "composed_chain.invoke({\"topic\":\"human\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e901b9b0",
   "metadata": {},
   "source": [
    "#### Now we have seen how to chain inputs and outputs with LLMs but what if we give wrong format inputs to LLM and LLM gives the wrong format outputs than actuall system is designed for ? lets do data validations for inputs and outputs using pydantic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3830ce1",
   "metadata": {},
   "source": [
    "## LangChain with Pydantic Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "094d8b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'products': [{'id': 'E101', 'name': 'Smartphone XYZ', 'price': 699.99},\n",
       "  {'id': 'E102', 'name': 'Ultra HD Smart TV', 'price': 1299.5},\n",
       "  {'id': 'E103', 'name': 'Gaming Laptop Pro', 'price': 1599.0},\n",
       "  {'id': 'E104',\n",
       "   'name': 'Wireless Noise-Cancelling Headphones',\n",
       "   'price': 249.99},\n",
       "  {'id': 'E105', 'name': 'Smartwatch Series 5', 'price': 199.99},\n",
       "  {'id': 'E106', 'name': 'Bluetooth Home Speaker', 'price': 99.99},\n",
       "  {'id': 'E107', 'name': 'Digital SLR Camera', 'price': 849.99},\n",
       "  {'id': 'E108', 'name': 'Drone with 4K Camera', 'price': 499.99},\n",
       "  {'id': 'E109', 'name': 'Portable Bluetooth Printer', 'price': 149.5},\n",
       "  {'id': 'E110', 'name': 'VR Headset', 'price': 299.99}]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using pydantic for the data type validation\n",
    "product_promt=ChatPromptTemplate([('system','you are a product DB and returns top {num_products}, product ID, product name and Product price, returns in {format}'),\n",
    "                                                (\"user\",'give 10 product list from product category type {product_category}')]\n",
    "                                                )\n",
    "product_chain= product_promt | llm | JsonOutputParser()\n",
    "product_chain.invoke({\"product_category\":'Electronicss',\n",
    "                      \"num_products\":10,\n",
    "                      \"format\":JsonOutputParser().get_format_instructions()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e7139d",
   "metadata": {},
   "source": [
    "#### Pydantic is a Python library that provides data validation and settings management using Python type hints. It ensures that data you pass into your models is valid, correctly typed, and well-structured—helping you write safer, more maintainable code. Pydantic works by parsing input data and converting it (when possible) into the correct types, raising an error if the data is invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6654991f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'products': [{'id': 'E001', 'name': 'Smartphone', 'price': 699.99},\n",
       "  {'id': 'E002', 'name': 'Laptop', 'price': 1099.99},\n",
       "  {'id': 'E003', 'name': 'Tablet', 'price': 499.99},\n",
       "  {'id': 'E004', 'name': 'Smartwatch', 'price': 299.99},\n",
       "  {'id': 'E005', 'name': 'Wireless Earbuds', 'price': 149.99},\n",
       "  {'id': 'E006', 'name': 'Bluetooth Speaker', 'price': 99.99},\n",
       "  {'id': 'E007', 'name': 'Gaming Console', 'price': 399.99},\n",
       "  {'id': 'E008', 'name': '4K Television', 'price': 899.99},\n",
       "  {'id': 'E009', 'name': 'Digital Camera', 'price': 549.99},\n",
       "  {'id': 'E010', 'name': 'Home Security Camera', 'price': 199.99}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the above example, lets try to validate the output Json schema\n",
    "from pydantic import BaseModel, Field\n",
    "class Product(BaseModel):\n",
    "    pid:str = Field(description=\"Product ID\")\n",
    "    pname:str = Field(description=\"Product Name \")\n",
    "    price:float = Field(description=\"Product Price\")\n",
    "\n",
    "json_pyparser= JsonOutputParser(pydantic_object=Product)\n",
    "\n",
    "product_chain= product_promt | llm | json_pyparser\n",
    "product_chain.invoke({\"product_category\":'Electronicss',\n",
    "                      \"num_products\":10,\n",
    "                      \"format\":JsonOutputParser().get_format_instructions()})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvagentic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
